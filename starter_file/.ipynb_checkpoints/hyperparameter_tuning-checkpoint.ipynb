{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "Import Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Model\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration. Make sure the config file is present at .\\config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'hdr-heart-experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcompute_cluster_name = \"heart-compute\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "For this project, I used the [Heart Failure dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) from Kaggle. \n",
    "\n",
    "Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"heart-failure\"\n",
    "\n",
    "dataset = ws.datasets[key]\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.take(5).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "\n",
    "\n",
    "# Create the different params that you will be using during training\n",
    "param_sampling = RandomParameterSampling(\n",
    "    {\n",
    "        '--C': choice(0.01, 0.1, 1.0, 10.0, 100.0),\n",
    "        '--max_iter': choice(20, 50, 100, 120, 150)\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Create your estimator and hyperdrive config\n",
    "estimator = SKLearn(source_directory='./', \n",
    "                    script='train.py',\n",
    "                    compute_target=compute_target)\n",
    "\n",
    "hdr = HyperDriveConfig(\n",
    "    estimator=estimator, \n",
    "    hyperparameter_sampling=param_sampling, \n",
    "    policy=early_termination_policy, \n",
    "    primary_metric_name='Accuracy', \n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=20, \n",
    "    max_concurrent_runs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit your experiment\n",
    "hdr = exp.submit(config=hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hdr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "Get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run = hdr.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Run Id : {}\".format(best_run.id), \n",
    "      \"Accuracy : {}\".format(best_run_metrics['Accuracy']), \n",
    "      \"Best metrics : {}\".format(best_run_metrics), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model = best_run.register_model(model_name='heart-failure-best-model-hdr', model_path='./outputs-hdr/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.download_file('outputs-hdr/conda_env_v_1_0_0.yml', 'conda_env.yml')\n",
    "environment = Environment.from_conda_specification(name='heart-failure-env', file_path='conda_env.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the scoring file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.download_file('outputs-hdr/scoring_file_v_1_0_0.py', 'score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(entry_script='score.py', environment=environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True, enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice = Model.deploy(workspace=ws, \n",
    "                        name='heart-failure-ws', \n",
    "                        models=[model], \n",
    "                        inference_config=inference_config,\n",
    "                        deployment_config=aci_config,\n",
    "                        overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for deployment to finish and display the scoring uri and swagger uri\n",
    "webservice.wait_for_deployment(show_output=True)\n",
    "\n",
    "print(\"Status : {}\".format(webservice.state), \n",
    "      \"scoring_uri : {}\".format(webservice.scoring_uri), \n",
    "      \"swagger_uri : {}\".format(webservice.swagger_uri), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume \n",
    "# NOT FINISHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples from the dataframe\n",
    "sample = df.sample(4)\n",
    "target = sample.pop('DEATH_EVENT')\n",
    "\n",
    "# convert the records to a json data file\n",
    "data_json = sample.to_dict(orient='records')\n",
    "\n",
    "data = json.dumps({'data': recored})\n",
    "\n",
    "# get inference\n",
    "output = webservice.run(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice.delete()\n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
